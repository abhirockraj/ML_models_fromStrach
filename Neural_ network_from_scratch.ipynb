{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 3, 4, 2, 6]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "np.random.seed(0)\n",
    "# ,[1, 2, 3, 2.5,1],[1, 2, 3, 2.5,1]\n",
    "\n",
    "X = [[1, 2, 3, 2.5,1]]\n",
    "\n",
    "X= np.reshape(X,(5,1))\n",
    "y_a = np.array([1,3,4,2,6])\n",
    "y_a= np.reshape(y,(1,5))\n",
    "y_a\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.7725886222397764\n"
     ]
    }
   ],
   "source": [
    "class Layer_Dense:\n",
    "    def __init__(self, n_neurons, n_inputs):\n",
    "        self.weights = 0.10 * np.random.randn(n_neurons, n_inputs)\n",
    "#         self.biases = np.zeros((1, n_neurons))\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.matmul( self.weights , inputs) \n",
    "\n",
    "class ActivationRelu:\n",
    "    def __init__(self, inputs):\n",
    "        self.output= np.maximum(0,inputs)\n",
    "        \n",
    "class Activation_softmax:\n",
    "    def __init__(self,inputs):\n",
    "        expV = np.exp(inputs - np.max(inputs,keepdims=True, axis=1) )\n",
    "        probabity = expV/np.sum(expV,keepdims=True, axis=1)\n",
    "        self.output= probabity\n",
    "class Loss:\n",
    "    def calculate (self, ot,y):\n",
    "        samples =self.forward(ot,y)\n",
    "        return np.mean(samples)\n",
    "\n",
    "class CrossEntropy(Loss):\n",
    "    def forward(self, y_p, y_a):\n",
    "        sample = len(y_p)\n",
    "        y_p = np.clip(y_p,1e-7,1-1e-7)\n",
    "        if(len(y_p.shape) == 1):\n",
    "            correct_config =  y_p[range(sample),y_a]\n",
    "        elif(len(y_p.shape) == 2):\n",
    "            correct_config =  np.sum(y_p*y_a, axis=1)\n",
    "        return (-np.log(correct_config))    \n",
    "        \n",
    "\n",
    "layer1 = Layer_Dense(3,5)\n",
    "layer2 = Layer_Dense(4,3)\n",
    "layer3 = Layer_Dense(3,4)\n",
    "# layer4 = Layer_Dense(1,2)\n",
    "\n",
    "layer1.forward(X)\n",
    "o1= layer1.output\n",
    "ao1=ActivationRelu(o1)\n",
    "# print(ao1.output,\"l1\")\n",
    "# print(layer1.output)\n",
    "layer2.forward(ao1.output)\n",
    "o2= layer2.output\n",
    "ao2 = ActivationRelu(o2)\n",
    "# print(ao2.output,\"l2\")\n",
    "layer3.forward(ao2.output)\n",
    "o3= layer3.output\n",
    "# ao3 = ActivationRelu(o3)\n",
    "# print(ao3.output,\"l3\")\n",
    "# layer4.forward(ao3.output)\n",
    "# o4= layer4.output\n",
    "# ao4 = ActivationRelu(o4)\n",
    "prob = Activation_softmax(o3)\n",
    "loss = CrossEntropy()\n",
    "l=loss.calculate(prob.output,y_a)\n",
    "# print(ao4.output.shape)\n",
    "# print(prob.output,\"softmax\")\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33323698, 0.33328147, 0.33329465])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= prob.output[range(len(prob.output)),[0,1,1]]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
